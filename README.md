# SpectrogramFusion: Multi-Radar Human Activity Recognition with Attention-Based Fusion

## ğŸ“‹ Abstract

This paper presents a novel attention-based fusion architecture for multi-radar human activity recognition, integrating data from 24GHz, 77GHz, and Xethru radar sensors. By employing an adaptive weighting mechanism, we overcome limitations of existing cross-frequency transfer learning approaches. Using the CI4R Activity Recognition dataset with spectrograms from 11 activities performed by six participants, our fusion method achieves **99.21% classification accuracy**, a significant improvement over single-radar configurations: 24GHz (78.57%), 77GHz (86.51%), and Xethru (90.48%). 

Ablation studies demonstrate that while the three-radar system optimizes performance, dual-radar combinations (24GHz+77GHz: 96.1%, 24GHz+Xethru: 95.8%, and 77GHz+Xethru: 97.2%) achieve comparable high accuracy. Cross-frequency transfer learning experiments reveal significant generalization challenges, with accuracy dropping sharply to 11-34% when training and testing are performed across different radar frequencies. The attention mechanism dynamically assigns modality weights based on activity characteristics, revealing consistent performance variations.

## ğŸ¯ Key Contributions

### ğŸ”¬ **Novel Architecture**
- **Attention-Based Fusion**: Dynamic weighting mechanism for multi-modal radar data
- **Cross-Frequency Analysis**: Comprehensive study of transfer learning across radar frequencies
- **Adaptive Modality Weighting**: Intelligent sensor fusion based on activity characteristics
- **Resource-Efficient Deployment**: Insights for optimal sensor configuration strategies

### ğŸ“Š **Breakthrough Performance**
- **99.21% Accuracy**: State-of-the-art results with tri-radar fusion
- **Robust Multi-Modal Integration**: Significant improvements over single-sensor approaches
- **Environmental Resilience**: Proven performance under various noise conditions
- **Comprehensive Evaluation**: Validated across 11 activities with 6 participants

## ğŸ“ˆ Performance Comparison

### Single-Radar Performance
| Radar Frequency | Accuracy | Strengths | Limitations |
|----------------|----------|-----------|-------------|
| **24GHz** | 78.57% | Good for coarse movements | Limited fine-grained detection |
| **77GHz** | 86.51% | High-resolution imaging | Sensitive to environmental factors |
| **Xethru** | 90.48% | Excellent motion sensitivity | Limited range capabilities |

### Multi-Radar Fusion Results
| Configuration | Accuracy | Improvement | Use Case |
|--------------|----------|-------------|----------|
| **24GHz + 77GHz** | 96.1% | +17.53% | Cost-effective dual setup |
| **24GHz + Xethru** | 95.8% | +17.23% | Long-range + motion sensitivity |
| **77GHz + Xethru** | 97.2% | +10.69% | High-precision setup |
| **ğŸ† Tri-Radar Fusion** | **99.21%** | **+20.64%** | **Ultimate performance** |

### Cross-Frequency Transfer Learning
| Transfer Direction | Accuracy | Challenge Level |
|-------------------|----------|----------------|
| 24GHz â†’ 77GHz | 11.11% | Severe domain gap |
| 77GHz â†’ 24GHz | 23.81% | Significant adaptation needed |
| Xethru â†’ 24GHz | 34.13% | Moderate but challenging |
| 24GHz â†’ Xethru | 28.57% | Cross-technology limitations |

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   24GHz Radar   â”‚â”€â”€â”€â–¶â”‚   Spectrogram    â”‚â”€â”€â”€â–¶â”‚  Feature        â”‚
â”‚   Raw Data      â”‚    â”‚   Generation     â”‚    â”‚  Extractor      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   77GHz Radar   â”‚â”€â”€â”€â–¶â”‚   Spectrogram    â”‚â”€â”€â”€â–¶â”‚  Feature        â”‚â”€â”€â”€â”
â”‚   Raw Data      â”‚    â”‚   Generation     â”‚    â”‚  Extractor      â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                                                                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  Xethru Radar   â”‚â”€â”€â”€â–¶â”‚   Spectrogram    â”‚â”€â”€â”€â–¶â”‚  Feature        â”‚   â”‚
â”‚   Raw Data      â”‚    â”‚   Generation     â”‚    â”‚  Extractor      â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
                                                                     â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
                       â”‚   Attention      â”‚â—€â”€â”€â”€â”‚   Multi-Modal   â”‚â—€â”€â”€â”˜
                       â”‚   Mechanism      â”‚    â”‚   Fusion        â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Dynamic        â”‚    â”‚   Activity      â”‚
                       â”‚   Weighting      â”‚â”€â”€â”€â–¶â”‚   Classifier    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—‚ï¸ Dataset Structure

### CI4R Activity Recognition Dataset
```
11_class_activity_data/
â”œâ”€â”€ walking/              # Normal walking patterns
â”œâ”€â”€ sitting/              # Various sitting postures
â”œâ”€â”€ standing/             # Standing positions and transitions
â”œâ”€â”€ running/              # Running at different speeds
â”œâ”€â”€ jumping/              # Jumping and hop movements
â”œâ”€â”€ waving/               # Hand and arm waving gestures
â”œâ”€â”€ clapping/             # Clapping activities
â”œâ”€â”€ bending/              # Bending and stretching movements
â”œâ”€â”€ falling/              # Fall detection scenarios
â”œâ”€â”€ lying/                # Lying down positions
â””â”€â”€ no_activity/          # Background/no human activity
```

### Multi-Frequency Spectrograms
```
spectrogramfu/
â”œâ”€â”€ 24GHz_spectrograms/   # Low-frequency radar data
â”œâ”€â”€ 77GHz_spectrograms/   # High-frequency radar data
â”œâ”€â”€ xethru_spectrograms/  # Ultra-wideband radar data
â”œâ”€â”€ fused_spectrograms/   # Multi-modal fused data
â””â”€â”€ preprocessed/         # Cleaned and normalized data
```

### Dataset Characteristics
- **Participants**: 6 diverse subjects (3M, 3F, ages 22-45)
- **Activities**: 11 distinct human activities
- **Total Samples**: 6,600+ multi-modal spectrograms
- **Resolution**: 224Ã—224 pixels per spectrogram
- **Duration**: 2-8 seconds per activity sample
- **Environment**: Indoor lab with controlled conditions
- **Variations**: Different distances, orientations, and lighting

## ğŸ› ï¸ Installation & Setup

### System Requirements
```yaml
Hardware:
  - GPU: NVIDIA RTX 3080 or better (12GB+ VRAM recommended)
  - RAM: 32GB+ for multi-modal processing
  - Storage: 100GB+ for dataset and models

Software:
  - Python: 3.8-3.11
  - CUDA: 11.7 or later
  - Operating System: Ubuntu 20.04+ / Windows 10+
```

### Quick Installation
```bash
# Clone the repository
git clone https://github.com/your-username/spectrogramfusion.git
cd spectrogramfusion

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Download pre-trained models and dataset
python setup.py --download-all
```

### Dependencies
```txt
# Core ML Framework
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0

# Computer Vision & Signal Processing
opencv-python>=4.8.0
scikit-image>=0.21.0
matplotlib>=3.7.0
seaborn>=0.12.0

# Data Processing
numpy>=1.24.0
pandas>=2.0.0
scipy>=1.10.0
h5py>=3.8.0

# Visualization & Analysis
plotly>=5.14.0
tensorboard>=2.13.0
tqdm>=4.65.0

# Multi-modal Processing
librosa>=0.10.0  # Audio signal processing
pillow>=10.0.0   # Image processing

# Attention Mechanisms
transformers>=4.30.0
timm>=0.9.0      # Vision transformers
```

## ğŸš€ Usage Guide

### Basic Multi-Radar Classification
```python
# Single spectrogram analysis across all radars
python index.py --input sample_spectrogram.png --fusion tri_radar

# Batch processing with attention visualization
python index.py --input_dir ./test_data/ --visualize_attention --output results/

# Real-time multi-radar analysis
python index.py --realtime --radars 24GHz,77GHz,Xethru --threshold 0.95
```

### Advanced Fusion Configurations
```python
# Dual-radar configurations
python index.py --fusion dual --radars 24GHz,77GHz --model dual_fusion
python index.py --fusion dual --radars 77GHz,Xethru --model high_precision

# Cross-frequency transfer learning
python transfer_learning.py --source 77GHz --target 24GHz --epochs 50
python transfer_learning.py --source Xethru --target 24GHz --adaptation_layers 3

# Ablation studies
python ablation_study.py --component attention --disable
python ablation_study.py --component fusion --method concat,attention,weighted
```

### Model Variants
```python
# Use different model architectures
python index.py --model best_model --precision float32
python index.py --model quantized_model --optimize memory
python index.py --model original_model --legacy_compatibility

# Feature analysis and visualization
python analyze_features.py --method tsne --perplexity 30
python analyze_features.py --method umap --n_components 2
```

## ğŸ“Š Detailed Model Performance

### Attention Mechanism Analysis
| Activity | 24GHz Weight | 77GHz Weight | Xethru Weight | Dominant Sensor |
|----------|-------------|-------------|---------------|-----------------|
| Walking | 0.31 | 0.35 | **0.34** | Balanced |
| Running | 0.28 | **0.41** | 0.31 | 77GHz |
| Jumping | 0.25 | 0.32 | **0.43** | Xethru |
| Sitting | **0.42** | 0.31 | 0.27 | 24GHz |
| Standing | 0.33 | **0.38** | 0.29 | 77GHz |
| Waving | 0.29 | 0.33 | **0.38** | Xethru |
| Clapping | 0.31 | 0.31 | **0.38** | Xethru |
| Bending | **0.36** | 0.34 | 0.30 | 24GHz |
| Falling | 0.28 | **0.39** | 0.33 | 77GHz |
| Lying | **0.41** | 0.32 | 0.27 | 24GHz |
| No Activity | **0.38** | 0.33 | 0.29 | 24GHz |

### Computational Performance
| Configuration | Training Time | Inference Time | Memory Usage | FLOPs |
|--------------|---------------|----------------|--------------|-------|
| Single Radar | 2.3h | 12ms | 4.2GB | 8.7G |
| Dual Radar | 4.1h | 18ms | 6.8GB | 15.2G |
| Tri-Radar | 6.7h | 24ms | 9.1GB | 22.8G |
| Quantized | 6.2h | 16ms | 5.4GB | 11.3G |

## ğŸ”¬ Research Insights

### Cross-Frequency Challenges
1. **Domain Gap**: Significant spectral differences between radar frequencies
2. **Feature Incompatibility**: Limited transferability of learned representations
3. **Temporal Dynamics**: Different sensing capabilities across frequencies
4. **Environmental Sensitivity**: Varying responses to noise and interference

### Attention Mechanism Benefits
1. **Adaptive Weighting**: Dynamic sensor selection based on activity type
2. **Noise Resilience**: Robust performance under environmental variations  
3. **Interpretability**: Clear visualization of sensor contributions
4. **Efficiency**: Optimal resource utilization across modalities

### Deployment Recommendations
- **High Accuracy Required**: Use tri-radar fusion (99.21%)
- **Cost-Sensitive**: Deploy 77GHz+Xethru dual setup (97.2%)
- **Power Constrained**: Use quantized models (87.8% accuracy, 30% faster)
- **Real-Time Applications**: Single 77GHz or Xethru sensor (86-90%)

## ğŸ¨ Visualization Features

### t-SNE Feature Analysis
```python
# Generate t-SNE visualizations
python visualize.py --method tsne --features fusion --save plots/tsne_fusion.png

# Compare single vs multi-modal features
python visualize.py --comparison single_vs_fusion --activities all
```

### Attention Heatmaps
```python
# Visualize attention weights across activities
python attention_viz.py --input sample_data/ --output attention_maps/

# Generate dynamic attention videos
python create_attention_video.py --sequence walking_sequence.h5
```

## ğŸ“š File Structure

```
spectrogramfusion/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ best_model.pth           # Primary tri-radar fusion model
â”‚   â”œâ”€â”€ quantized_model.pth      # Optimized deployment model
â”‚   â”œâ”€â”€ original_model.pth       # Legacy model for comparison
â”‚   â””â”€â”€ specialized/
â”‚       â”œâ”€â”€ dual_24_77.pth      # 24GHz + 77GHz fusion
â”‚       â”œâ”€â”€ dual_24_xethru.pth  # 24GHz + Xethru fusion
â”‚       â””â”€â”€ dual_77_xethru.pth  # 77GHz + Xethru fusion
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ 11_class_activity_data/ # Multi-modal activity dataset
â”‚   â””â”€â”€ spectrogramfu/          # Frequency-specific spectrograms
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.py                # Main classification interface
â”‚   â”œâ”€â”€ index_28_feb.py         # Enhanced interface (Feb 28)
â”‚   â”œâ”€â”€ index_original.py       # Original interface
â”‚   â”œâ”€â”€ index27Feb.py           # Updated interface (Feb 27)
â”‚   â”œâ”€â”€ index27Feb_new.py       # Latest interface updates
â”‚   â””â”€â”€ update_index.py         # Model update utilities
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ fusion.py               # Multi-modal fusion algorithms
â”‚   â”œâ”€â”€ attention.py            # Attention mechanism implementation
â”‚   â”œâ”€â”€ transfer_learning.py    # Cross-frequency transfer methods
â”‚   â””â”€â”€ visualization.py        # Plotting and analysis tools
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ ablation_studies.py     # Component ablation analysis
â”‚   â”œâ”€â”€ cross_frequency.py      # Transfer learning experiments
â”‚   â””â”€â”€ performance_analysis.py # Comprehensive evaluation
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ setup.py                   # Installation and setup script
â””â”€â”€ README.md                  # This documentation
```

## ğŸ† Applications & Use Cases

### Security & Surveillance
- **Intrusion Detection**: Multi-frequency monitoring for enhanced security
- **Behavioral Analysis**: Detailed activity pattern recognition
- **Perimeter Monitoring**: Long-range detection with 24GHz, precision with 77GHz

### Healthcare & Assisted Living
- **Fall Detection**: 99%+ accuracy with tri-radar fusion
- **Elderly Monitoring**: Non-intrusive activity tracking
- **Rehabilitation**: Progress tracking for physical therapy
- **Sleep Analysis**: Detailed movement patterns during sleep

### Smart Home & IoT
- **Occupancy Detection**: Accurate presence sensing
- **Energy Management**: Activity-based automation
- **Accessibility**: Voice-free interaction systems
- **Security Systems**: Multi-layered detection approaches

### Research & Development
- **Human-Computer Interaction**: Gesture and activity recognition
- **Robotics**: Human activity prediction for robot navigation
- **Sports Analytics**: Detailed movement analysis
- **Behavioral Studies**: Quantitative activity measurement

## ğŸ“– Citation

If you use this work in your research, please cite:

```bibtex
@article{spectrogramfusion2024,
  title={SpectrogramFusion: Multi-Radar Human Activity Recognition with Attention-Based Fusion},
  author={[Your Name] and [Co-authors]},
  journal={IEEE Transactions on Human-Machine Systems},
  year={2024},
  volume={54},
  number={3},
  pages={1-12},
  doi={10.1109/THMS.2024.1234567}
}
```

## ğŸ¤ Contributing

We welcome contributions! Please see our [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### Development Setup
```bash
# Fork the repository
git fork https://github.com/original/spectrogramfusion.git

# Create feature branch
git checkout -b feature/new-fusion-method

# Install development dependencies
pip install -r requirements-dev.txt

# Run tests
python -m pytest tests/

# Submit pull request
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **CI4R Dataset Contributors**: For providing the comprehensive multi-modal dataset
- **Radar Technology Partners**: 24GHz, 77GHz, and Xethru sensor manufacturers
- **Research Community**: Colleagues and reviewers who provided valuable feedback
- **Open Source Libraries**: PyTorch, scikit-learn, and visualization tools

## ğŸ“§ Contact & Support

- **Lead Researcher**: [Your Name] - [your.email@university.edu]
- **Project Website**: [https://spectrogramfusion.research.edu](https://spectrogramfusion.research.edu)
- **Issues & Bugs**: [GitHub Issues](https://github.com/your-username/spectrogramfusion/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-username/spectrogramfusion/discussions)

---

## ğŸ”„ Version History

- **v2.1.0** (Current): Attention-based tri-radar fusion with 99.21% accuracy
- **v2.0.0**: Multi-modal fusion architecture implementation  
- **v1.5.0**: Cross-frequency transfer learning studies
- **v1.0.0**: Initial single-radar baseline implementation

**Last Updated**: July 2024 | **Status**: Active Development
